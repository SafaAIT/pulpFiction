{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU-dOdYvx1cV",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "113ca587-cf67-40ce-fe14-86f247b16d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U langchain transformers bitsandbytes accelerate gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community langchain-core"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5VDkXzvC5QA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d89781-7e6d-4320-ab95-c9ae51453b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (0.2.10)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.6 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.6)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.83)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.4.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (2.7.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.6->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.6 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import libraries"
      ],
      "metadata": {
        "id": "_Z87D1lCx4WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, pipeline"
      ],
      "metadata": {
        "id": "ZqyZxf3Ex9_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization config"
      ],
      "metadata": {
        "id": "3bEgN3OjyBhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "W5d2HUlbyCJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get user hf token key (modify if using colab secret)\n",
        "\n",
        "*   this should be created on your HuggingFace account, in the settings\n",
        "*   can be modified by using secrets on your colab\n",
        "\n"
      ],
      "metadata": {
        "id": "AvgO5Eey2wni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "Ak9N2Qd-2tPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4bits model and tokenizer"
      ],
      "metadata": {
        "id": "KpIrzVHryHSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained( \"mistralai/Mistral-7B-Instruct-v0.3\", device_map=\"auto\",quantization_config=quantization_config, )\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")"
      ],
      "metadata": {
        "id": "kWc27VXByJaI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VQUcl1Xo4MBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nZxfUiwF4r7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ],
      "metadata": {
        "id": "DBCt145O42Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    task='text-generation',\n",
        "    stopping_criteria=stopping_criteria,\n",
        "    temperature=0.7,  #  for more creative generation\n",
        "    max_new_tokens=1500,\n",
        "    # min_new_tokens=1000,\n",
        "    repetition_penalty=1.2  # reduce repetition\n",
        ")"
      ],
      "metadata": {
        "id": "z4oP7M1E5D6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# Define the LLMs\n",
        "llm_first = HuggingFacePipeline(pipeline=generate_text)\n",
        "llm_second = HuggingFacePipeline(pipeline=generate_text)\n",
        "llm_third = HuggingFacePipeline(pipeline=generate_text)\n",
        "llm_fourth = HuggingFacePipeline(pipeline=generate_text)\n",
        "\n",
        "# Define the prompt templates for each LLM\n",
        "template_first = \"\"\"\n",
        "The Lester Dent Pulp Paper Master Fiction Plot Formula is a writing guide created by Lester Dent, a prolific pulp fiction writer best known for his work on the \"Doc Savage\" series. Dent's formula provides a structured approach to crafting engaging and action-packed stories. Here is a brief overview of the formula:\n",
        "\n",
        "Introduction:\n",
        "Introduce the hero and the central problem or conflict.\n",
        "Set up a situation that hooks the reader's interest immediately.\n",
        "First Quarter:\n",
        "The hero tries to solve the problem.\n",
        "Introduce complications and obstacles that thwart the hero's initial attempts.\n",
        "Introduce other key characters (both allies and antagonists).\n",
        "End with a twist or a surprising development.\n",
        "\n",
        "Characters:\n",
        "- Hero: {hero}\n",
        "- Villain: {villain}\n",
        "- Sidekick: {sidekick}\n",
        "- Victim: {victim}\n",
        "- Witness: {witness}\n",
        "\n",
        "You are a story generator that follows the Lester Dent Pulp Paper formula.\n",
        "Based on this formula, your task is to generate exactly the first 1500 words of the story in one continuous block of text. The text should be coherent, engaging, and flow like a real story without any interruptions or section headers.\n",
        "\"\"\"\n",
        "\n",
        "template_second = \"\"\"\n",
        "The Lester Dent Pulp Paper Master Fiction Plot Formula continues as follows:\n",
        "\n",
        "Second Quarter:\n",
        "Escalate the conflict and introduce additional problems for the hero.\n",
        "The hero faces greater challenges and setbacks.\n",
        "Include physical conflicts or action scenes.\n",
        "Introduce a major plot twist or revelation that complicates the hero's mission.\n",
        "\n",
        "This is the first part of the story:\n",
        "{first_part}\n",
        "\n",
        "Based on this formula, your task is to generate the next 1500 words of the story in one continuous block of text. The text should be coherent, engaging, and flow like a real story without any interruptions or section headers.\n",
        "\"\"\"\n",
        "\n",
        "template_third = \"\"\"\n",
        "The Lester Dent Pulp Paper Master Fiction Plot Formula continues as follows:\n",
        "\n",
        "Third Quarter:\n",
        "The hero makes some progress towards solving the problem but faces significant adversity.\n",
        "Introduce new conflicts and obstacles.\n",
        "The hero encounters the main villain or a critical turning point in the story.\n",
        "End with a twist that puts the hero in an even worse situation.\n",
        "\n",
        "This is the first part of the story:\n",
        "{first_part}\n",
        "\n",
        "This is the second part of the story:\n",
        "{second_part}\n",
        "\n",
        "Based on this formula, your task is to generate the next 1500 words of the story in one continuous block of text. The text should be coherent, engaging, and flow like a real story without any interruptions or section headers.\n",
        "\"\"\"\n",
        "\n",
        "template_fourth = \"\"\"\n",
        "The Lester Dent Pulp Paper Master Fiction Plot Formula continues as follows:\n",
        "\n",
        "Final Quarter:\n",
        "The hero faces the greatest challenges and is pushed to their limits.\n",
        "All mysteries and plot threads are resolved.\n",
        "The hero uses their skills, intelligence, and bravery to overcome the final obstacles.\n",
        "End with a final twist or surprise that concludes the story in a satisfying way.\n",
        "\n",
        "This is the first part of the story:\n",
        "{first_part}\n",
        "\n",
        "This is the second part of the story:\n",
        "{second_part}\n",
        "\n",
        "This is the third part of the story:\n",
        "{third_part}\n",
        "\n",
        "Based on this formula, your task is to generate the final 1500 words of the story which is the last quarter of the story so you should end the story in this chapter. The generated text should be in one continuous block of text. The text should be coherent, engaging, and flow like a real story without any interruptions or section headers.\n",
        "\"\"\"\n",
        "\n",
        "# Function to combine all parts into one text\n",
        "def combine_story_parts(*parts):\n",
        "    return \"\\n\\n\".join(parts)\n",
        "\n",
        "# Function to generate the first part\n",
        "def generate_first_part(hero, villain, sidekick, victim, witness):\n",
        "    prompt_first = PromptTemplate.from_template(template_first)\n",
        "    first_result = llm_first(prompt_first.format(hero=hero, villain=villain, sidekick=sidekick, victim=victim, witness=witness))\n",
        "    return first_result\n",
        "\n",
        "# Function to generate the second part\n",
        "def generate_second_part(first_part):\n",
        "    prompt_second = PromptTemplate.from_template(template_second)\n",
        "    second_result = llm_second(prompt_second.format(first_part=first_part))\n",
        "    return second_result\n",
        "\n",
        "# Function to generate the third part\n",
        "def generate_third_part(first_part, second_part):\n",
        "    prompt_third = PromptTemplate.from_template(template_third)\n",
        "    third_result = llm_third(prompt_third.format(first_part=first_part, second_part=second_part))\n",
        "    return third_result\n",
        "\n",
        "# Function to generate the final part\n",
        "def generate_final_part(first_part, second_part, third_part):\n",
        "    prompt_fourth = PromptTemplate.from_template(template_fourth)\n",
        "    fourth_result = llm_fourth(prompt_fourth.format(first_part=first_part, second_part=second_part, third_part=third_part))\n",
        "    return fourth_result\n",
        "\n",
        "# Function to combine all parts and generate the full story\n",
        "def generate_full_story(hero, villain, sidekick, victim, witness, first_part, second_part, third_part):\n",
        "    # Generate the final part\n",
        "    final_part = generate_final_part(first_part, second_part, third_part)\n",
        "\n",
        "    # Combine all parts\n",
        "    full_story = combine_story_parts(first_part, second_part, third_part, final_part)\n",
        "    return full_story\n",
        "\n",
        "# Define the Gradio interface\n",
        "with gr.Blocks() as interface:\n",
        "    gr.Markdown(\"# Story Generator\")\n",
        "    gr.Markdown(\"Generate a complete story based on the Lester Dent Pulp Paper formula.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        hero_input = gr.Textbox(label=\"Hero\")\n",
        "        villain_input = gr.Textbox(label=\"Villain\")\n",
        "        sidekick_input = gr.Textbox(label=\"Sidekick\")\n",
        "        victim_input = gr.Textbox(label=\"Victim\")\n",
        "        witness_input = gr.Textbox(label=\"Witness\")\n",
        "        generate_first = gr.Button(\"Generate First Part\")\n",
        "\n",
        "    first_part_output = gr.Textbox(label=\"First Part\", lines=10)\n",
        "\n",
        "    with gr.Row():\n",
        "        generate_second = gr.Button(\"Generate Second Part\")\n",
        "        first_part_modified = gr.Textbox(label=\"Modified First Part\", lines=10)\n",
        "\n",
        "    second_part_output = gr.Textbox(label=\"Second Part\", lines=10)\n",
        "\n",
        "    with gr.Row():\n",
        "        generate_third = gr.Button(\"Generate Third Part\")\n",
        "        second_part_modified = gr.Textbox(label=\"Modified Second Part\", lines=10)\n",
        "\n",
        "    third_part_output = gr.Textbox(label=\"Third Part\", lines=10)\n",
        "\n",
        "    with gr.Row():\n",
        "        generate_final = gr.Button(\"Generate Final Part\")\n",
        "        third_part_modified = gr.Textbox(label=\"Modified Third Part\", lines=10)\n",
        "\n",
        "    final_part_output = gr.Textbox(label=\"Final Part\", lines=10)\n",
        "\n",
        "    generate_full = gr.Button(\"Generate Full Story\")\n",
        "    full_story_output = gr.Textbox(label=\"Full Story\", lines=20)\n",
        "\n",
        "    # Set up the interactions\n",
        "    generate_first.click(\n",
        "        fn=generate_first_part,\n",
        "        inputs=[hero_input, villain_input, sidekick_input, victim_input, witness_input],\n",
        "        outputs=[first_part_output]\n",
        "    )\n",
        "\n",
        "    generate_second.click(\n",
        "        fn=generate_second_part,\n",
        "        inputs=[first_part_output],\n",
        "        outputs=[second_part_output]\n",
        "    )\n",
        "\n",
        "    generate_third.click(\n",
        "        fn=generate_third_part,\n",
        "        inputs=[first_part_modified, second_part_output],\n",
        "        outputs=[third_part_output]\n",
        "    )\n",
        "\n",
        "    generate_final.click(\n",
        "        fn=generate_final_part,\n",
        "        inputs=[first_part_modified, second_part_modified, third_part_modified],\n",
        "        outputs=[final_part_output]\n",
        "    )\n",
        "\n",
        "    generate_full.click(\n",
        "        fn=generate_full_story,\n",
        "        inputs=[hero_input, villain_input, sidekick_input, victim_input, witness_input, first_part_modified, second_part_modified, third_part_modified],\n",
        "        outputs=[full_story_output]\n",
        "    )\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "XL0VSDGL5gFc",
        "outputId": "9e1da125-abd8-425b-99b0-7abea073408e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://89193b4f405d4c4d8f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://89193b4f405d4c4d8f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}